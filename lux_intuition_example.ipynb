{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./pyuid3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyuid3.uncertain_entropy_evaluator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyuid3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Data\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyuid3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01muid3\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UId3\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyuid3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01muncertain_entropy_evaluator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UncertainEntropyEvaluator\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneighbors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NearestNeighbors\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyuid3.uncertain_entropy_evaluator'"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification, make_moons, make_circles, make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc, confusion_matrix, classification_report\n",
    "from sklearn import preprocessing\n",
    "import sklearn\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from anchor import utils\n",
    "from anchor import anchor_tabular\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import normalize, MinMaxScaler\n",
    "import shap\n",
    "import lime\n",
    "from pyuid3.data import Data\n",
    "from pyuid3.uid3 import UId3\n",
    "from pyuid3.uncertain_entropy_evaluator import UncertainEntropyEvaluator\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmaxdf(df, scale=[-1,1]):\n",
    "    # Using MinMaxScaler\n",
    "    min_max_scaler = MinMaxScaler(feature_range=scale)    \n",
    "    # Stack everything into a single column to scale by the global min / max\n",
    "    tmp = df.to_numpy().reshape(-1,1)\n",
    "    scaled = min_max_scaler.fit_transform(tmp).reshape(len(df), df.shape[1])\n",
    "    return scaled\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = 1000\n",
    "neighborhood_size=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create synthetic classification dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = sklearn.datasets.make_moons(n_samples=dataset_size,  noise=0.25, random_state=0)#factor=0.5\n",
    "#Xa,ya = sklearn.datasets.make_blobs(n_samples=100, centers=1,cluster_std=0.2, n_features=2,\n",
    "                                    #random_state=0, center_box=(-0.8,0))\n",
    "#Xb,yb = sklearn.datasets.make_blobs(n_samples=100, centers=1,cluster_std=0.2, n_features=2,\n",
    "                                   # random_state=0, center_box=(-3.5,3.3))\n",
    "\n",
    "#X = np.vstack((X,Xa,Xb))\n",
    "#y = np.hstack((y,ya,yb+1))\n",
    "\n",
    "Xdf = pd.DataFrame(X, columns=['x1','x2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(pd.DataFrame(X), y, test_size=0.33, random_state=42)\n",
    "X_train.columns = ['x1','x2']\n",
    "X_test.columns = ['x1','x2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.plot(kind='scatter',x='x1', y='x2',style='.', c=y_train, colormap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel='rbf',probability=True)\n",
    "svc.fit(X_train, y_train)\n",
    "svc_preds = svc.predict(X_test)\n",
    "print(accuracy_score(y_test, svc_preds))\n",
    "\n",
    "print(classification_report(y_test, svc_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "xclf = LogisticRegression()\n",
    "xclf.fit(X_train, y_train)\n",
    "lr_preds = xclf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test, lr_preds))\n",
    "\n",
    "print(classification_report(y_test, lr_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "point = [1.0,0.0]\n",
    "ax.plot(point[0], point[1], 'or', markersize=8)\n",
    "X_test.plot(kind='scatter',x='x1', y='x2', c=y_test, colormap='viridis', ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_appended = X_test.append(pd.DataFrame([point], columns=['x1','x2']), ignore_index=True)\n",
    "y_test_appended = np.concatenate([y_test,[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "h = .02  # step size in the mesh\n",
    "# create a mesh to plot in\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "Z = svc.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "ZZ = xclf.predict(pd.DataFrame(np.c_[xx.ravel(), yy.ravel()], columns=['x1','x2']))\n",
    "\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "fig,ax= plt.subplots(1,2, figsize=(12,3))\n",
    "ax[0].contour(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "Xdf.plot(kind='scatter',x='x1', y='x2',style='.', c=y, colormap='viridis',ax=ax[0])\n",
    "\n",
    "ZZ = ZZ.reshape(xx.shape)\n",
    "ax[1].contour(xx, yy, ZZ, cmap=plt.cm.Paired)\n",
    "X_train.plot(kind='scatter',x='x1', y='x2',style='.', c=y_train, colormap='viridis',ax=ax[1])\n",
    "ax[0].set(title='SVM decision boundary')\n",
    "ax[1].set(title='XGBoost decision boundary')\n",
    "fig.savefig('dataset.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "from matplotlib import colors,cm\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12,3))\n",
    "\n",
    "y_test_proba = svc.predict_proba(X_test_appended)\n",
    "confidence_svc =   pd.DataFrame(MinMaxScaler().fit_transform(pd.DataFrame({'SVC_0':y_test_proba[:,0], 'SVC_1':y_test_proba[:,1]})))\n",
    "\n",
    "y_test_proba_xg = xclf.predict_proba(X_test_appended)\n",
    "confidence_xgb =   pd.DataFrame(MinMaxScaler().fit_transform(pd.DataFrame({'XGB_0':y_test_proba_xg[:,0],'XGB_1':y_test_proba_xg[:,1]})))\n",
    "\n",
    "X_test_1 = X_test_appended.iloc[y_test_appended==1]\n",
    "X_test_2 = X_test_appended.iloc[y_test_appended==0]\n",
    "X_test_1.columns=['x1','x2']\n",
    "\n",
    "cmapp = cm.viridis\n",
    "\n",
    "\n",
    "c=0\n",
    "cmap = colors.LinearSegmentedColormap.from_list(\n",
    "        'incr_alpha', [(0, (*tuple(cmapp.colors[c]),0)), (1, cmapp.colors[c])])\n",
    "ax[0].scatter(X_test_1['x1'], X_test_1['x2'], c=confidence_svc.iloc[y_test_appended==1,1].values, cmap=cmap)#, ec=None, s=10**2)\n",
    "\n",
    "\n",
    "c=int(255*.5)\n",
    "cmap = colors.LinearSegmentedColormap.from_list(\n",
    "        'incr_alpha', [(0, (*tuple(cmapp.colors[c]),0)), (1, cmapp.colors[c])])\n",
    "ax[0].scatter(X_test_2['x1'], X_test_2['x2'], c=confidence_svc.iloc[y_test_appended==0,0].values, cmap=cmap)#, ec=None, s=10**2)\n",
    "ax[0].set(title='SVM uncertainty in predicions')\n",
    "\n",
    "\n",
    "c=0\n",
    "\n",
    "cmap = colors.LinearSegmentedColormap.from_list(\n",
    "        'incr_alpha', [(0, (*tuple(cmapp.colors[c]),0)), (1, cmapp.colors[c])])\n",
    "ax[1].scatter(X_test_1['x1'], X_test_1['x2'], c=confidence_xgb.iloc[y_test_appended==1,1].values, cmap=cmap)#, ec=None, s=10**2)\n",
    "\n",
    "\n",
    "c=int(255*.5)\n",
    "cmap = colors.LinearSegmentedColormap.from_list(\n",
    "        'incr_alpha', [(0, (*tuple(cmapp.colors[c]),0)), (1, cmapp.colors[c])])\n",
    "ax[1].scatter(X_test_2['x1'], X_test_2['x2'], c=confidence_xgb.iloc[y_test_appended==0,0].values, cmap=cmap)#, ec=None, s=10**2)\n",
    "ax[1].set(title='XGB uncertainty in predictions')\n",
    "\n",
    "ax[0].plot(point[0], point[1], 'or', markersize=8)\n",
    "ax[1].plot(point[0], point[1], 'or', markersize=8)\n",
    "\n",
    "\n",
    "fig.savefig('uncertainty-point.png')\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "# create a mesh to plot in\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "Z = svc.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "ZZ = xclf.predict(pd.DataFrame(np.c_[xx.ravel(), yy.ravel()], columns=['x1','x2']))\n",
    "\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "fig,ax= plt.subplots(1,2, figsize=(12,3))\n",
    "ax[0].contour(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "ax[0].scatter(X_test['x1'], X_test['x2'], c=y_test, cmap='viridis')\n",
    "\n",
    "ZZ = ZZ.reshape(xx.shape)\n",
    "ax[1].contour(xx, yy, ZZ, cmap=plt.cm.Paired)\n",
    "ax[1].scatter(X_test['x1'], X_test['x2'], c=y_test, cmap='viridis')\n",
    "ax[0].set(title='SVM decision boundary')\n",
    "ax[1].set(title='XGBoost decision boundary')\n",
    "\n",
    "ax[0].plot(point[0], point[1], 'or', markersize=8)\n",
    "ax[1].plot(point[0], point[1], 'or', markersize=8)\n",
    "fig.savefig('decbound-point.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lime explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values, feature_names=list(X_train.columns), class_names=['0','1'], discretize_continuous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = len(X_test_appended)-1\n",
    "exp = explainer.explain_instance(X_test_appended.iloc[idx,:], svc.predict_proba, num_features=2, top_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook(show_table=True, show_all=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shap explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# use Kernel SHAP to explain test set predictions\n",
    "explainer = shap.KernelExplainer(svc.predict_proba, X_train, link=\"logit\")\n",
    "shap_values = explainer.shap_values(X_test_appended, nsamples=100)\n",
    "\n",
    "# plot the SHAP values for the Setosa output of the first instance\n",
    "idx = len(X_test_appended)-1\n",
    "shap.force_plot(explainer.expected_value[0], shap_values[0][idx,:], X_test_appended.iloc[idx,:], link=\"logit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anchor explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = anchor_tabular.AnchorTabularExplainer(\n",
    "    ['0','1'],\n",
    "    feature_names = X_train.columns,\n",
    "    train_data = X_train.values,\n",
    "    categorical_names={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "print('Prediction: ', explainer.class_names[svc.predict(X_test_appended.iloc[idx].values.reshape(1,-1))[0]])\n",
    "exp = explainer.explain_instance(X_test_appended.iloc[idx].values, svc.predict, threshold=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Anchor: %s' % (' AND '.join(exp.names())))\n",
    "print('Precision: %.2f' % exp.precision())\n",
    "print('Coverage: %.2f' % exp.coverage())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LUX explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select neighbourhood for explaned points (from train) (in this cas epoint only for the sake of simplicity)\n",
    "# Use KNN to sample K-neighbours for each class\n",
    "# export dataset(s) for UID3 and obtain rules\n",
    "# highlight rule triggered for point example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NearestNeighbors(n_neighbors=neighborhood_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pos_only = X_train[y_train==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.fit(X_pos_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,ids_1 = nn.kneighbors(np.array(point).reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_neg_only = X_train[y_train==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.fit(X_neg_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,ids_0 = nn.kneighbors(np.array(point).reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sample = pd.concat((X_pos_only.iloc[ids_1.ravel()], X_neg_only.iloc[ids_0.ravel()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_sample = svc.predict_proba(X_train_sample)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighbourhood plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "from matplotlib import colors,cm\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12,3))\n",
    "\n",
    "y_test_proba = svc.predict_proba(X_train_sample)\n",
    "confidence_svc =   pd.DataFrame(MinMaxScaler().fit_transform(pd.DataFrame({'SVC_0':y_test_proba[:,0], 'SVC_1':y_test_proba[:,1]})))\n",
    "\n",
    "#y_test_proba_xg = xclf.predict_proba(X_train_sample)\n",
    "#confidence_xgb =   pd.DataFrame(MinMaxScaler().fit_transform(pd.DataFrame({'XGB_0':y_test_proba_xg[:,0],'XGB_1':y_test_proba_xg[:,1]})))\n",
    "\n",
    "y_test_proba_o = svc.predict_proba(X_test_appended)\n",
    "confidence_svc_o =   pd.DataFrame(MinMaxScaler().fit_transform(pd.DataFrame({'SVC_0':y_test_proba_o[:,0], \n",
    "                                                                             'SVC_1':y_test_proba_o[:,1]})))\n",
    "\n",
    "#y_test_proba_xg_o = xclf.predict_proba(X_test_appended)\n",
    "#confidence_xgb_o =   pd.DataFrame(MinMaxScaler().fit_transform(pd.DataFrame({'XGB_0':y_test_proba_xg_o[:,0],\n",
    "#                                                                             'XGB_1':y_test_proba_xg_o[:,1]})))\n",
    "\n",
    "\n",
    "y_tr_sam_bin = (y_test_proba[:,1]>=0.5).astype('int')\n",
    "y_tr_sam_bin_o = (y_test_proba_o[:,1]>=0.5).astype('int')\n",
    "\n",
    "X_test_1_o = X_test_appended.iloc[y_tr_sam_bin_o==1]\n",
    "X_test_2_o = X_test_appended.iloc[y_tr_sam_bin_o==0]\n",
    "X_test_1_o.columns=['x1','x2']\n",
    "\n",
    "X_test_1 = X_train_sample.iloc[y_tr_sam_bin==1]\n",
    "X_test_2 = X_train_sample.iloc[y_tr_sam_bin==0]\n",
    "X_test_1.columns=['x1','x2']\n",
    "\n",
    "cmapp = cm.viridis\n",
    "\n",
    "\n",
    "c=0\n",
    "cmap = colors.LinearSegmentedColormap.from_list(\n",
    "        'incr_alpha', [(0, (*tuple(cmapp.colors[c]),0)), (1, cmapp.colors[c])])\n",
    "ax[0].scatter(X_test_1_o['x1'], X_test_1_o['x2'], c=confidence_svc_o.iloc[y_tr_sam_bin_o==1,1].values, cmap=cmap)#, ec=None, s=10**2)\n",
    "\n",
    "\n",
    "c=int(255*.5)\n",
    "cmap = colors.LinearSegmentedColormap.from_list(\n",
    "        'incr_alpha', [(0, (*tuple(cmapp.colors[c]),0)), (1, cmapp.colors[c])])\n",
    "ax[0].scatter(X_test_2_o['x1'], X_test_2_o['x2'], c=confidence_svc_o.iloc[y_tr_sam_bin_o==0,0].values, cmap=cmap)#, ec=None, s=10**2)\n",
    "ax[0].set(title='SVM uncertainty in predicions for test set')\n",
    "\n",
    "#rect = patches.Rectangle((0.5,-0.5),1,1,angle=0,ec='r', fc=\"None\", ls='--', linewidth=2)\n",
    "rect = patches.Circle(tuple(point),0.7,ec='r', fc=\"None\", ls='--', linewidth=2)\n",
    "\n",
    "ax[0].add_patch(rect)\n",
    "\n",
    "\n",
    "c=0\n",
    "\n",
    "cmap = colors.LinearSegmentedColormap.from_list(\n",
    "        'incr_alpha', [(0, (*tuple(cmapp.colors[c]),0)), (1, cmapp.colors[c])])\n",
    "ax[1].scatter(X_test_1['x1'], X_test_1['x2'], c=(confidence_svc.iloc[y_tr_sam_bin==1,1].values), cmap=cmap)#, ec=None, s=10**2)\n",
    "\n",
    "\n",
    "c=int(255*.5)\n",
    "cmap = colors.LinearSegmentedColormap.from_list(\n",
    "        'incr_alpha', [(0, (*tuple(cmapp.colors[c]),0)), (1, cmapp.colors[c])])\n",
    "ax[1].scatter(X_test_2['x1'], X_test_2['x2'], c=confidence_svc.iloc[y_tr_sam_bin==0,0].values, cmap=cmap)#, ec=None, s=10**2)\n",
    "ax[1].set(title='SVM uncertainty in predictions for neighbourhood $N$')\n",
    "\n",
    "ax[0].plot(point[0], point[1], 'or', markersize=8)\n",
    "ax[1].plot(point[0], point[1], 'or', markersize=8)\n",
    "#rect = patches.Rectangle((0.5,-0.5),1,1,angle=0,ec='r', fc=\"None\", ls='--', linewidth=2)\n",
    "#ax[1].add_patch(rect)\n",
    "\n",
    "fig.savefig('neighbourhood.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To UARFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lux.numeric.arff', 'w') as file:\n",
    "    file.write('@relation lux\\n\\n')\n",
    "    file.write('@attribute x1 @REAL\\n')\n",
    "    file.write('@attribute x2 @REAL\\n')\n",
    "    file.write('@attribute class {1,0}\\n\\n')\n",
    "    \n",
    "    file.write('@data\\n')\n",
    "    for i in range(0, len(X_train_sample)):\n",
    "        file.write('{:.2f}'.format(X_train_sample.iloc[i,0])+'[1],'+'{:.2f}'.format(X_train_sample.iloc[i,1])+'[1],1['+'{:.2f}'.format(y_train_sample[i])+']\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Uncertain Decision tree for whole decision boundary\n",
    "Every instance can be explained with a path on this tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data.parse_uarff(\"./lux.numeric.arff\")\n",
    "uid3 = UId3(max_depth=2)\n",
    "tree = uid3.fit(data, entropyEvaluator=UncertainEntropyEvaluator(), depth=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.save_dot('explanation.dot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "graphviz.Source.from_file('explanation.dot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tree.to_HMR())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HMR](explanations.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
